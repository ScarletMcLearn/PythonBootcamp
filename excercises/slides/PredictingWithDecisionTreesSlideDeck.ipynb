{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Titanic: Machine Learning from Disaster</h1></center>\n",
    "![Titanic](img/titanic.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to decision trees\n",
    "- In the previous chapter we found subsets of individuals that have a higher chance of surviving. \n",
    "- A decision tree automates this process for you and outputs a classification model or classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Starts with all the data at the root node\n",
    "- Scans all the variables for the best one to split on. \n",
    "- Once a variable is chosen, you do the split and go down one level (or one node) and repeat. \n",
    "- The final nodes at the bottom of the decision tree are known as terminal nodes\n",
    "- The majority vote of the observations in that node determine how to predict for new observations that end up in that terminal node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Decision Tree](img/decisionTree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Instructions \n",
    "\n",
    "- Import the <b>numpy</b> library as <b>np</b>\n",
    "- From <b>sklearn</b> import the <b>tree</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import the Numpy library\n",
    "\n",
    "# Import 'tree' from scikit-learn library\n",
    "from sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import the Numpy library\n",
    "import numpy as np\n",
    "\n",
    "# Import 'tree' from scikit-learn library\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cleaning and Formatting your Data\n",
    "\n",
    "- Before you can start we have to clean up our data so we can use all features available to us\n",
    "- The Age variable includes missing values\n",
    "- Substitute missing values by the median of all present values\n",
    "\n",
    "\n",
    "    train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())\n",
    "\n",
    "- The <b>Sex</b> and <b>Embarked</b> variables include missing values, but are non numeric\n",
    "- You can assign each class in <b>Sex</b> an unique integer\n",
    "- and substitute the missing values in <b>Embarked</b> by with the most common class,which is <b>\"S\"</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Instructions\n",
    "\n",
    "- Assign the integer 1 to all females\n",
    "- Impute missing values in <b>Embarked</b> with class <b>S</b> and in <b>Age</b> by its median. Use <b>.fillna()</b> method.\n",
    "- Replace each class of Embarked with a uniques integer. <b>0</b> for <b>S</b>, <b>1</b> for <b>C</b>, and <b>2</b> for <b>Q</b>.\n",
    "- Print the Sex and Embarked columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "########################## Ignore this part #####################################\n",
    "import pandas as pd\n",
    "\n",
    "# Load the train and test datasets to create two DataFrames\n",
    "train_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "train = pd.read_csv(train_url)\n",
    "\n",
    "test_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\"\n",
    "test = pd.read_csv(test_url)\n",
    "#################################################################################\n",
    "\n",
    "# Convert the male and female groups to integer form\n",
    "train.ix[train.Sex == \"male\", \"Sex\"] = 0\n",
    "\n",
    "# Impute the Embarked variable\n",
    "train[\"Embarked\"] = \n",
    "\n",
    "# Convert the Embarked classes to integer form\n",
    "train.ix[train.Embarked == \"S\", \"Embarked\"] = 0\n",
    "\n",
    "# Print the Sex and Embarked columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "########################## Ignore this part #####################################\n",
    "import pandas as pd\n",
    "\n",
    "# Load the train and test datasets to create two DataFrames\n",
    "train_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "train = pd.read_csv(train_url)\n",
    "\n",
    "test_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\"\n",
    "test = pd.read_csv(test_url)\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "# Convert the male and female groups to integer form\n",
    "train.ix[train.Sex == \"male\", \"Sex\"] = 0\n",
    "train.ix[train.Sex == \"female\", \"Sex\"] = 1\n",
    "\n",
    "# Impute the Embarked variable\n",
    "train[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "# Impute the Age variable\n",
    "train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())\n",
    "\n",
    "# Convert the Embarked classes to integer form\n",
    "train.ix[train.Embarked == \"S\", \"Embarked\"] = 0\n",
    "train.ix[train.Embarked == \"C\", \"Embarked\"] = 1\n",
    "train.ix[train.Embarked == \"Q\", \"Embarked\"] = 2\n",
    "\n",
    "#Print the Sex and Embarked columns\n",
    "print(train[[\"Sex\", \"Embarked\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating your first decision tree\n",
    "- Build your first decision tree, using scikit-learn and numpy. \n",
    "- Tree objects can be generated using the DecisionTreeClassifier class. \n",
    "- The methods that we will use take numpy arrays as inputs \n",
    "- We will reformat the DataFrame that we already have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will need the following to build a decision tree\n",
    "\n",
    "- <b>target</b>: A one-dimensional numpy array containing the target/response from the train data. (<b>Survival</b> in your case)\n",
    "- <b>features</b>: A multidimensional numpy array containing the features/predictors from the train data. (ex. <b>Sex</b>, Age)\n",
    "\n",
    "Take a look at the sample code below to see what this would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "target = train[\"Survived\"].values\n",
    "features = train[[\"Sex\", \"Age\"]].values\n",
    "my_tree = tree.DecisionTreeClassifier()\n",
    "my_tree = my_tree.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Sneak peak: See the importance of the features that are included in the decision tree\n",
    "- This can be done by requesting the <b>.feature\\_importances_</b> attribute of your tree object\n",
    "- Another quick metric is the mean accuracy that you can compute using the <b>.score()</b> function with features_one and target as arguments.\n",
    "\n",
    "Ok, time for you to build your first decision tree in Python! The train and testing data from chapter 1 are available in your workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "- Build the <b>target</b> and <b>features_one</b> numpy arrays. The target will be based on the <b>Survived</b> column in <b>train</b>. The features array will be based on the variables Passenger Class, Sex, Age, and Passenger Fare\n",
    "- Build a decision tree <b>my_tree_one</b> to predict survival using <b>features_one</b> and <b>target</b>\n",
    "- Look at the importance of features in your tree and compute the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Print the train data to see the available features\n",
    "print(train)\n",
    "\n",
    "# Create the target and features numpy arrays: target, features_one\n",
    "target = train[___].values\n",
    "features_one = train[[\"Pclass\", \"Sex\", \"Age\", \"Fare\"]].values\n",
    "\n",
    "# Fit your first decision tree: my_tree_one\n",
    "my_tree_one = tree.DecisionTreeClassifier()\n",
    "my_tree_one = my_tree_one.fit(___, ___)\n",
    "\n",
    "# Look at the importance and score of the included features\n",
    "print(my_tree_one.feature_importances_)\n",
    "print(my_tree_one.score(___, ___))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Print the train data to see the available features\n",
    "#print(train.head())\n",
    "\n",
    "# Create the target and features numpy arrays: target, features_one\n",
    "target = train[\"Survived\"].values\n",
    "features_one = train[[\"Pclass\", \"Sex\", \"Age\", \"Fare\"]].values\n",
    "\n",
    "# Fit your first decision tree: my_tree_one\n",
    "my_tree_one = tree.DecisionTreeClassifier()\n",
    "my_tree_one = my_tree_one.fit(features_one, target)\n",
    "\n",
    "# Look at the importance and score of the included features\n",
    "#print()\n",
    "#print(my_tree_one.feature_importances_)\n",
    "#print(my_tree_one.score(features_one, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpeting your decision tree\n",
    "- The <b>feature\\_importances_</b> attribute make it simple to interpret the significance of the predictors you include.\n",
    "- What variable plays the most important role in determining whether or not a passenger survived? \n",
    "\n",
    "Have a look at your model (<b>my_tree_one</b>) to find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. [ ] Passenger Class\n",
    "\n",
    "2. [ ] Sex/Gender\n",
    "\n",
    "3. [ ] Passenger Fare\n",
    "\n",
    "4. [ ] Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. [ ] Passenger Class\n",
    "\n",
    "2. [ ] Sex/Gender\n",
    "\n",
    "3. [ x ] Passenger Fare\n",
    "\n",
    "4. [ ] Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predict on unseen data\n",
    "- In the last excercise we created simple predictions based on a single subset. \n",
    "- We can make use of some simple functions to \"generate\" our answer without having to manually perform subsetting.\n",
    "\n",
    "1. First, you make use of the <b>.predict()</b> method. \n",
    "2. You provide it the model (<b>my_tree_one</b>), the values of features from the dataset for which predictions need to be made (<b>test</b>). \n",
    "3. Extract features using a numpy array (same as for training)\n",
    "\n",
    "<b>BUT NOT SO FAST</b>: There are missing values in the <b>Fare</b> and <b>Age</b> feature, which will have to be imputed first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "- Impute the missing value for Fare and Age with the median of the respective column.\n",
    "- Make a prediction on the test set using the <b>.predict()</b> method and <b>my_tree_one</b>. Assign the result to <b>my_prediction</b>.\n",
    "- Create a data frame <b>my_solution</b> containing the solution and the passenger ids from the test set. Make sure the solution is in line with the standards set forth by Kaggle by naming the column appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "########################## Ignore this part ######################################\n",
    "test.ix[test.Sex == \"male\", \"Sex\"] = 0\n",
    "test.ix[test.Sex == \"female\", \"Sex\"] = 1\n",
    "#################################################################################\n",
    "\n",
    "# Impute the missing value with the median\n",
    "test[\"Fare\"] = \n",
    "test[\"Age\"] =\n",
    "\n",
    "# Extract the features from the test set: Pclass, Sex, Age, and Fare.\n",
    "test_features = test[[___, ___, ___, ___]].values\n",
    "\n",
    "# Make your prediction using the test set\n",
    "my_prediction = my_tree_one.predict(test_features)\n",
    "\n",
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions\n",
    "PassengerId = np.array(test[\"PassengerId\"]).astype(int)\n",
    "my_solution = pd.DataFrame(my_prediction, PassengerId, columns = [\"Survived\"])\n",
    "print(my_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "########################## Ignore this part #####################################\n",
    "test.ix[test.Sex == \"male\", \"Sex\"] = 0\n",
    "test.ix[test.Sex == \"female\", \"Sex\"] = 1\n",
    "#################################################################################\n",
    "\n",
    "# Impute the missing value with the median\n",
    "test[\"Fare\"] = test[\"Fare\"].fillna(test[\"Fare\"].median())\n",
    "test[\"Age\"] = test[\"Age\"].fillna(test[\"Age\"].median())\n",
    "\n",
    "# Extract the features from the test set: Pclass, Sex, Age, and Fare.\n",
    "test_features = test[[\"Pclass\", \"Sex\", \"Age\", \"Fare\"]].values\n",
    "\n",
    "# Make your prediction using the test set\n",
    "my_prediction = my_tree_one.predict(test_features)\n",
    "print(my_prediction)\n",
    "\n",
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions\n",
    "PassengerId = np.array(test[\"PassengerId\"]).astype(int)\n",
    "my_solution = pd.DataFrame(my_prediction, PassengerId, columns = [\"Survived\"])\n",
    "print(my_solution.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting and how to control it\n",
    "\n",
    "- Using default parameters for <b>max_depth</b> and <b>min_samples_split</b> is likely to overfit our data\n",
    "- Our model describes the training data extremely well, but it does not generalize to new data\n",
    "- We can improve by setting thresholds for <b>max_depth</b> and <b>min_samples_split</b>\n",
    "- <b>max_depth</b>: Determines when the splitting up of the decision tree stops.\n",
    "- <b>min_samples_split</b>: Monitors the amount of observations in a bucket (e.g minimum 10 passengers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Instructions\n",
    "\n",
    "- Include the Siblings/Spouses Aboard, Parents/Children Aboard, and Embarked features in a new set of features.\n",
    "- Fit your second tree <b>my_tree_two</b> with the new features, and control for the model compelexity by toggling the <b>max_depth</b> and <b>min_samples_split</b> arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new array with the added features: features_two\n",
    "features_two = train[[\"Pclass\",\"Age\",\"Sex\",\"Fare\", ___, ___, ___]].values\n",
    "\n",
    "#Control overfitting by setting \"max_depth\" to 10 and \"min_samples_split\" to 5 : my_tree_two\n",
    "max_depth = \n",
    "min_samples_split =\n",
    "my_tree_two = tree.DecisionTreeClassifier(max_depth = ___, min_samples_split = ____, random_state = 1)\n",
    "my_tree_two = \n",
    "\n",
    "#Print the score of the new decison tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new array with the added features: features_two\n",
    "features_two = train[[\"Pclass\",\"Age\",\"Sex\",\"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "# Control overfitting by setting \"max_depth\" to 10 and \"min_samples_split\" to 5 : my_tree_two\n",
    "max_depth = 10\n",
    "min_samples_split = 5\n",
    "my_tree_two = tree.DecisionTreeClassifier(max_depth = max_depth, min_samples_split = min_samples_split, random_state = 1)\n",
    "my_tree_two = my_tree_two.fit(features_two, target)\n",
    "my_tree_two_score = my_tree_two.score(features_two, target)\n",
    "\n",
    "# Print the score of the new decison tree\n",
    "print(my_tree_two_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature-Engineering for our Titanic data set\n",
    "While feature engineering is a discipline in itself, too broad to be covered here in detail, you will have a look at a simple example by creating your own new predictive attribute: <b>family_size</b>.\n",
    "\n",
    "Assumption: \"Larger families need more time to get together on a sinking ship\"\n",
    "Hence: Lower chance of survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Family size is determined by the variables <b>SibSp</b> and <b>Parch</b>\n",
    "- Add a new variable <b>family_size</b>, which is the sum of <b>SibSp</b> and - <b>Parch</b> plus one (the observation itself), to the test and train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Instructions\n",
    "- Create a new train set train_two that differs from train only by having an extra column with your feature engineered variable family_size.\n",
    "- Add your feature engineered variable <b>family_size</b> in addition to <b>Pclass</b>, <b>Sex</b>, <b>Age</b>, <b>Fare</b>, <b>SibSp</b> and <b>Parch</b> to <b>features_three</b>.\n",
    "- Create a new decision tree as <b>my_tree_three</b> and fit the decision tree with your new feature set <b>features_three</b>. Then check out the score of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create train_two with the newly defined feature\n",
    "train_two = train.copy()\n",
    "train_two[\"family_size\"] = \n",
    "\n",
    "# Create a new feature set and add the new feature\n",
    "features_three = train_two[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"SibSp\", \"Parch\", ___]].values\n",
    "\n",
    "# Define the tree classifier, then fit the model\n",
    "my_tree_three = tree.DecisionTreeClassifier()\n",
    "my_tree_three = \n",
    "\n",
    "# Print the score of this decision tree\n",
    "print(my_tree_three.score(features_three, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create train_two with the newly defined feature\n",
    "train_two = train.copy()\n",
    "train_two[\"family_size\"] = train_two[[\"SibSp\", \"Parch\"]].sum(axis=1) + 1\n",
    "\n",
    "# Create a new feature set and add the new feature\n",
    "features_three = train_two[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"SibSp\", \"Parch\", \"family_size\"]].values\n",
    "\n",
    "# Define the tree classifier, then fit the model\n",
    "my_tree_three = tree.DecisionTreeClassifier()\n",
    "my_tree_three = my_tree_three.fit(features_three, target)\n",
    "\n",
    "# Print the score of this decision tree\n",
    "print(my_tree_three.score(features_three, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Random Forest analysis in Python\n",
    "A detailed study of Random Forests would take this tutorial a bit too far. However, since it's an often used machine learning technique, gaining a general understanding in Python won't hurt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In layman's terms, the Random Forest technique handles the overfitting problem you faced with decision trees. \n",
    "\n",
    "- Grows multiple (very deep) classification trees using the training set. \n",
    "- Each tree is used to come up with a prediction\n",
    "- Every outcome is counted as a vote. \n",
    "\n",
    "For example, if you have trained 3 trees with 2 saying a passenger in the test set will survive and 1 says he will not, the passenger will be classified as a survivor. \n",
    "\n",
    "This approach of overtraining trees, but having the majority's vote count as the actual classification decision, avoids overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Use <b>RandomForestClassifier()</b> class instead of the <b>DecisionTreeClassifier()</b> class.\n",
    "- <b>n_estimators</b> needs to be set when using the <b>RandomForestClassifier()</b> class. This argument allows you to set the number of trees you wish to plant and average over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Instructions\n",
    "\n",
    "- Build the random forest with n_estimators set to 100.\n",
    "- Fit your random forest model with inputs features_forest and target.\n",
    "- Compute the classifier predictions on the selected test set features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "########################## Ignore this part ######################################\n",
    "test.ix[test.Embarked == \"S\", \"Embarked\"] = 0\n",
    "test.ix[test.Embarked == \"C\", \"Embarked\"] = 1\n",
    "test.ix[test.Embarked == \"Q\", \"Embarked\"] = 2\n",
    "##################################################################################\n",
    "\n",
    "# Import the `RandomForestClassifier`\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# We want the Pclass, Age, Sex, Fare,SibSp, Parch, and Embarked variables\n",
    "features_forest = train[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "# Building and fitting my_forest\n",
    "forest = RandomForestClassifier(max_depth = 10, min_samples_split=2, n_estimators = ___, random_state = 1)\n",
    "my_forest = forest.fit(___, ___)\n",
    "\n",
    "# Print the score of the fitted random forest\n",
    "print(my_forest.score(features_forest, target))\n",
    "\n",
    "# Compute predictions on our test set features then print the length of the prediction vector\n",
    "test_features = test[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "pred_forest = my_forest.predict(___)\n",
    "print(pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Ignore this part\n",
    "test.ix[test.Embarked == \"S\", \"Embarked\"] = 0\n",
    "test.ix[test.Embarked == \"C\", \"Embarked\"] = 1\n",
    "test.ix[test.Embarked == \"Q\", \"Embarked\"] = 2\n",
    "################################################################################\n",
    "\n",
    "# Import the `RandomForestClassifier`\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# We want the Pclass, Age, Sex, Fare,SibSp, Parch, and Embarked variables\n",
    "features_forest = train[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "# Building and fitting my_forest\n",
    "forest = RandomForestClassifier(max_depth = 10, min_samples_split=2, n_estimators = 100, random_state = 1)\n",
    "my_forest = forest.fit(features_forest, target)\n",
    "\n",
    "# Print the score of the fitted random forest\n",
    "print(my_forest.score(features_forest, target))\n",
    "\n",
    "# Compute predictions on our test set features then print the length of the prediction vector\n",
    "test_features = test[[\"Pclass\", \"Age\", \"Sex\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "pred_forest = my_forest.predict(test_features)\n",
    "print(pred_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpreting and Comparing\n",
    "Remember how we looked at <b>.feature\\_importances_</b> attribute for the decision trees? \n",
    "\n",
    "- Well, you can request the same attribute from your random forest as well \n",
    "- You might also want to compare the models in some quick and easy way. For this, we can use the <b>.score()</b> method. \n",
    "\n",
    "For this exercise, you have <b>my_forest</b> and <b>my_tree_two</b> available to you. The features and target arrays are also ready for use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Instructions\n",
    "- Explore the feature importance for both models\n",
    "- Compare the mean accuracy score of the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Request and print the `.feature_importances_` attribute\n",
    "print(my_tree_two.feature_importances_)\n",
    "print()\n",
    "\n",
    "# Compute and print the mean accuracy score for both models\n",
    "print(my_tree_two.score(features_two, target))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Request and print the `.feature_importances_` attribute\n",
    "print(my_tree_two.feature_importances_)\n",
    "print(my_forest.feature_importances_)\n",
    "\n",
    "#Compute and print the mean accuracy score for both models\n",
    "print(my_tree_two.score(features_two, target))\n",
    "print(my_forest.score(features_two, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conclude and Submit\n",
    "Based on your finding in the previous exercise determine which feature was of most importance, and for which model. After this final exercise. \n",
    "\n",
    "Use <b>my_forest</b>, <b>my_tree_two</b>, and <b>feature\\_importances_</b> to answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. [ ] The most important feature was \"<b>Age</b>\", but it was more significant for \"<b>my_tree_two</b>\"\n",
    "\n",
    "2. [ ] The most important feature was \"<b>Sex</b>\", but it was more significant for \"<b>my_tree_two</b>\"\n",
    "\n",
    "3. [ ] The most important feature was \"<b>Sex</b>\", but it was more significant for \"<b>my_forest</b>\"\n",
    "\n",
    "4. [ ] The most important feature was \"<b>Age</b>\", but it was more significant for \"<b>my_forest</b>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. [ ] The most important feature was \"<b>Age</b>\", but it was more significant for \"<b>my_tree_two</b>\"\n",
    "\n",
    "2. [ x ] The most important feature was \"<b>Sex</b>\", but it was more significant for \"<b>my_tree_two</b>\"\n",
    "\n",
    "3. [ ] The most important feature was \"<b>Sex</b>\", but it was more significant for \"<b>my_forest</b>\"\n",
    "\n",
    "4. [ ] The most important feature was \"<b>Age</b>\", but it was more significant for \"<b>my_forest</b>\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
